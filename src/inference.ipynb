{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference notebook to create run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "import cv2\n",
    "from models.retinaface import RetinaFace\n",
    "from utils.box_utils import decode\n",
    "from fastai.vision import *\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"C:/Repositories/dockship-mask-detection-2/input\")\n",
    "output_dir = Path(\"C:/Repositories/dockship-mask-detection-2/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "cfg = cfg_mnet\n",
    "cpu = True\n",
    "origin_size = True\n",
    "confidence_threshold = 0.02\n",
    "nms_threshold = 0.4\n",
    "save_folder = output_dir\n",
    "save_image = True\n",
    "vis_thres = 0.5\n",
    "resize = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from ./weights/mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n",
      "Finished loading model!\n"
     ]
    }
   ],
   "source": [
    "net = RetinaFace(cfg=cfg, phase = 'test')\n",
    "net = load_model(net, './weights/mobilenet0.25_Final.pth', cpu)\n",
    "net.eval()\n",
    "print('Finished loading model!')\n",
    "# print(net)\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cpu\" if cpu else \"cuda\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_bboxes():\n",
    "    good_dets = {}\n",
    "    for image in tqdm(os.listdir(input_dir)):\n",
    "        img_raw = cv2.imread(str(input_dir/image), cv2.IMREAD_COLOR)\n",
    "        img = np.float32(img_raw)\n",
    "        \n",
    "        if resize != 1:\n",
    "            img = cv2.resize(img, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "        im_height, im_width, _ = img.shape\n",
    "        scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "        img -= (104, 117, 123)\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        scale = scale.to(device)\n",
    "\n",
    "        loc, conf, _ = net(img)  # forward pass\n",
    "\n",
    "        priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "        priors = priorbox.forward()\n",
    "        priors = priors.to(device)\n",
    "        prior_data = priors.data\n",
    "        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        boxes = boxes * scale / resize\n",
    "        boxes = boxes.cpu().numpy()\n",
    "        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "\n",
    "        # ignore low scores\n",
    "        inds = np.where(scores > confidence_threshold)[0]\n",
    "        boxes = boxes[inds]\n",
    "        # landms = landms[inds]\n",
    "        scores = scores[inds]\n",
    "\n",
    "        # keep top-K before NMS\n",
    "        order = scores.argsort()[::-1]\n",
    "        # order = scores.argsort()[::-1][:args.top_k]\n",
    "        boxes = boxes[order]\n",
    "        # landms = landms[order]\n",
    "        scores = scores[order]\n",
    "\n",
    "        # do NMS\n",
    "        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "        keep = py_cpu_nms(dets, nms_threshold)\n",
    "        # keep = nms(dets, args.nms_threshold,force_cpu=args.cpu)\n",
    "        dets = dets[keep, :]\n",
    "        # landms = landms[keep]\n",
    "\n",
    "        good_det = []\n",
    "        for b in dets:\n",
    "            if b[4] < vis_thres:\n",
    "                continue\n",
    "            good_det.append(b)\n",
    "\n",
    "        good_dets[image] = good_det\n",
    "\n",
    "    return good_dets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"D:/Datasets/COVID-19-mask-detection/models\")\n",
    "learn = load_learner(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c818e61b229b46cb956001315f033422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "good_dets = get_face_bboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedc935f972b4bc082dde10ca30890ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for image in tqdm(list(good_dets.keys())):\n",
    "\n",
    "    img = PIL.Image.open(input_dir/image)\n",
    "    w, h = img.size\n",
    "    preds = []\n",
    "\n",
    "#     if(len(good_dets[image]) < 1):\n",
    "#         continue\n",
    "    \n",
    "    for box in good_dets[image]:\n",
    "        scale = 0.5\n",
    "\n",
    "        xpixels = int((box[2] - box[0]) * scale)\n",
    "        ypixels = int((box[3] - box[1]) * scale)\n",
    "\n",
    "        xmin = max(box[0] - xpixels, 0.)\n",
    "        ymin = max(box[1] - ypixels, 0.)\n",
    "        xmax = min(box[2] + xpixels, w)\n",
    "        ymax = min(box[3] + ypixels, h)\n",
    "\n",
    "        crop_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "        new_img = crop_img.resize((224,224))\n",
    "\n",
    "        img_tensor = T.ToTensor()((new_img).convert(\"RGB\"))\n",
    "        img_fastai = Image(img_tensor)\n",
    "        try:\n",
    "            preds.append(learn.predict(img_fastai))\n",
    "        except:\n",
    "            print(image, img.size, (xmin, ymin, xmax, ymax))\n",
    "\n",
    "    cats = [\"face\", \"mask\"]\n",
    "    img_raw = cv2.imread(str(input_dir/image), cv2.IMREAD_COLOR)\n",
    "    for i, b in enumerate(good_dets[image]):\n",
    "        if int(preds[i][0]): color = (0, 255, 0)\n",
    "        else: color = (0, 0, 255)\n",
    "        cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), color, 2);\n",
    "        cx = int(b[0])\n",
    "        cy = int(b[1] + 12)\n",
    "        text = f\"{cats[int(preds[i][0])]}\" + \" {:.4f}\".format(b[4])\n",
    "        cv2.putText(img_raw, text, (cx, cy),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    name = str(output_dir/image)\n",
    "    cv2.imwrite(name, img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai-torch1.1] *",
   "language": "python",
   "name": "conda-env-fastai-torch1.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
